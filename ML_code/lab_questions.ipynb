{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Familiarize basic python packages for deep learning such as Keras, Tensorflow etc.\n",
        "2. Data pre-processing operations such as outliers and/or inconsistent data value\n",
        "management. **\n",
        "3. Implement Feed forward neural network with three hidden layers for classification\n",
        "on CIFAR-10 dataset.**\n",
        "4. Analyse the impact of optimization and weight initialization techniques such as\n",
        "Xavier initialization, Kaiming Initialization, dropout and regularization techniques\n",
        "and visualize the change in performance. **\n",
        "5. Digit classification using CNN architecture for MNIST dataset. **\n",
        "6. Digit classification using pre-trained networks like VGGnet-19 for MNIST dataset\n",
        "and analyse and visualize performance improvement.**\n",
        "7. Implement a simple RNN for review classification using IMDB dataset.**\n",
        "8. Analyse and visualize the performance change while using LSTM and GRU\n",
        "instead of simple RNN.**\n",
        "9. Implement time series forecasting prediction for NIFTY-50 dataset. **\n",
        "10. Implement a shallow auto encoder and decoder network for machine translation(by\n",
        "using Kaggle English to Hindi neural translation dataset"
      ],
      "metadata": {
        "id": "CUvbf_zrhqZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "left - 3 4 7\n",
        "\n",
        "done - 2 5 8 9 6"
      ],
      "metadata": {
        "id": "EKaDadngV_gL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 8,9"
      ],
      "metadata": {
        "id": "bEVEhqGUImhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmWKySzqIlXb"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Flatten, LSTM, GRU\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "sunspots = pd.read_csv('/content/Sunspots.csv', index_col='Date', parse_dates=True)\n",
        "sc = MinMaxScaler(feature_range = (0,1))\n",
        "data = sc.fit_transform(sunspots).flatten()\n",
        "data.shape\n",
        "train = data[:5330]\n",
        "test = data[5330:]\n",
        "\n",
        "\n",
        "n_input = 7\n",
        "n_features = 1\n",
        "\n",
        "timetrain = TimeseriesGenerator(train, train, length=n_input, batch_size=1 )\n",
        "timetest = TimeseriesGenerator(test, test, length=n_input, batch_size=1 )\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=100, input_shape=(n_input,n_features) , activation='relu',))\n",
        "# model.add(LSTM(units=100, input_shape=(n_input,n_features) , activation='relu',))\n",
        "# model.add(SimpleRNN(units=100, input_shape=(n_input,n_features) , activation='relu',))\n",
        "model.add(Dense(units=1, activation='relu'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(timetrain, epochs=5)\n",
        "predict = model.predict(timetest)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1RILEXkoPH_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 5"
      ],
      "metadata": {
        "id": "r-02By36PEQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using MNIST dataset\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "len(set(y_test.flatten()))\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, num_classes = 10)\n",
        "y_test = to_categorical(y_test, num_classes = 10)\n",
        "#Preprocessing\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "#Normalize pixel values to the range [0,1] (pixel values range from 0 to 255)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "#Finding input size\n",
        "X_train.shape\n",
        "\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "#Block1\n",
        "model.add(Conv2D(filters=25, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "#Block2\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "#block3\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "#categorical_crossentropy is used for multi class classification tasks\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "7lkqXCteBKmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes = 10)\n",
        "y_test = to_categorical(y_test, num_classes = 10)\n",
        "y_train\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "# X_train\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 25, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = (32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 1, validation_data = (X_test, y_test))\n",
        "score = model.evaluate(X_test, y_test, batch_size = 128, verbose = 0)\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "7ZwoKS__PDyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 6"
      ],
      "metadata": {
        "id": "UnDbqlsFV20o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "(X_train, y_train), (X_test,y_test) = tfds.load('tf_flowers', split=[\"train[:70%]\",\"train[30%:]\"], batch_size=-1, as_supervised=True)\n",
        "\n",
        "\n",
        "X_train = tf.image.resize(X_train,size=(150,150))\n",
        "X_test = tf.image.resize(X_test,size=(150,150))\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=5)\n",
        "y_test = to_categorical(y_test, num_classes=5)\n",
        "\n",
        "base_model = VGG16(weights='imagenet',include_top=False,input_shape=X_train[0].shape)\n",
        "base_model.trainable = False\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=50, activation='relu'))\n",
        "model.add(Dense(units=20, activation='relu'))\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_loss\", mode=\"max\", patience=5, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1, callbacks=[es])\n",
        "test_pred=model.predict(X_test)\n",
        "score= model.evaluate(X_test,y_test, batch_size=128, verbose=0)\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "CmZ9ikEyV5dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 7\n"
      ],
      "metadata": {
        "id": "24rcWgZNfXbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e\n",
        "from keras.datasets import imdb\n",
        "\n",
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))\n",
        "print('---review---', X_train[6])\n",
        "print('---label---', y_train[6])\n",
        "\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---',[id2word.get(i, ' ') for i in X_train[6]])\n",
        "print('---label---', y_train[6])\n",
        "print('Maximum review length: {}'.format(len(max((X_train + X_test), key=len))))\n",
        "print('Minimum review length: {}'.format(len(min((X_test + X_test), key=len))))\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "max_words = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = pad_sequences(X_test, maxlen=max_words)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "embedding_size=32\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "K=X_test\n",
        "K1=X_test[1]\n",
        "model.predict(K1)\n",
        "X_train2[0]\n"
      ],
      "metadata": {
        "id": "tTzRerbEfYT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"IMDB_Dataset.csv\")\n",
        "\n",
        "data.head()\n",
        "\n",
        "print(data.review[0])\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# html =\n",
        "# tags = r'[/<(?:\"[^\"]*\"['\"]*|'[^']*'['\"]*|[^'\">])+>/g;]'\n",
        "print(re.sub(\"<[^>]*>\", ' ', data.review[0]))\n",
        "tags_removed = re.sub(\"<[^>]*>\", ' ', data.review[0])\n",
        "\n",
        "html = r'[^0-9A-Za-z\\s]'\n",
        "print(re.sub(html, ' ', tags_removed))\n",
        "alpha = re.sub(html, ' ', tags_removed)\n",
        "\n",
        "pip install nltk\n",
        "\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "tokens = tokenizer.tokenize(alpha)\n",
        "print(tokens)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemWords = [stemmer.stem(word) for word in tokens]\n",
        "print(stemWords)\n",
        "\n",
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(stopwords)\n",
        "print(len(stopwords))\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "filtered_words = []\n",
        "for token in stemWords:\n",
        "    if token not in stopwords:\n",
        "        filtered_words.append(token)\n",
        "print(filtered_words)\n",
        "print(len(filtered_words))"
      ],
      "metadata": {
        "id": "5yTLlTfhghRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualization"
      ],
      "metadata": {
        "id": "7CRSKW4KBJ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_result(trainY, testY, train_predict, test_predict):\n",
        "    actual = np.append(trainY, testY)\n",
        "    predictions = np.append(train_predict, test_predict)\n",
        "    rows = len(actual)\n",
        "    plt.figure(figsize=(15, 6), dpi=80)\n",
        "    plt.plot(range(rows), actual)\n",
        "    plt.plot(range(rows), predictions)\n",
        "    plt.axvline(x=len(trainY), color='r')\n",
        "    plt.legend(['Actual', 'Predictions'])\n",
        "    plt.xlabel('Observation number after given time steps')\n",
        "    plt.ylabel('Sunspots scaled')\n",
        "    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n",
        "plot_result(trainY, testY, train_predict, test_predict)"
      ],
      "metadata": {
        "id": "x6K3UkS5K5iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thcA6eWDghEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}